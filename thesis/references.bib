@book{bratko2001prolog,
  title={Prolog programming for artificial intelligence},
  author={Bratko, Ivan},
  year={2001},
  publisher={Pearson education}
}
Ivan Bratko - Strukur des Progamms ist lose auf seiner Skizze aufgebaut

@book{Alpaydin+2022,
url = {https://doi.org/10.1515/9783110740196},
title = {Maschinelles Lernen},
author = {Ethem Alpaydin},
publisher = {De Gruyter Oldenbourg},
address = {Berlin, Boston},
doi = {doi:10.1515/9783110740196},
isbn = {9783110740196},
year = {2022},
lastchecked = {2022-08-29}
}
Abbildung Entscheidungsbaum

@Article{MOLA1997,
author={MOLA, FRANCESCO
and SICILIANO, ROBERTA},
title={A fast splitting procedure for classification trees},
journal={Statistics and Computing},
year={1997},
month={Sep},
day={01},
volume={7},
number={3},
pages={209-216},
abstract={This paper provides a faster method to find the best split at each node when using the CART methodology. The predictability index {\"I}„ is proposed as a splitting rule for growing the same classification tree as CART does when using the Gini index of heterogeneity as an impurity measure. A theorem is introduced to show a new property of the index {\"I}„: the {\"I}„ for a given predictor has a value not lower than the {\"I}„ for any split generated by the predictor. This property is used to make a substantial saving in the time required to generate a classification tree. Three simulation studies are presented in order to show the computational gain in terms of both the number of splits analysed at each node and the CPU time. The proposed splitting algorithm can prove computational efficiency in real data sets as shown in an example.},
issn={1573-1375},
doi={10.1023/A:1018590219790},
url={https://doi.org/10.1023/A:1018590219790}
}
schnellerer splitting Algorithmus


@article{Demidova_2020,
	doi = {10.1088/1742-6596/1479/1/012085},
	url = {https://doi.org/10.1088/1742-6596/1479/1/012085},
	year = 2020,
	month = {mar},
	publisher = {{IOP} Publishing},
	volume = {1479},
	number = {1},
	pages = {012085},
	author = {L A Demidova and P O Usachev},
	title = {Development and approbation of the improved {CART} algorithm version},
	journal = {Journal of Physics: Conference Series},
	abstract = {This article considers the classification problems using the decision trees. It suggests the improved CART algorithm version called clean CART (CCART). The key feature of this algorithm is the reducing of the hardware memory for the tree storing. The main ideas of the CCART algorithm software implementation have been presented. The experimental results of comparison of the original and the presented algorithms for constructing the decision trees confirm that the proposed CCART algorithm has the stated advantage. Therefore, the proposed CCART algorithm is recommended for use in the ensembles of classifiers construction.}
}
schlägt vor den Baum bis zur completten Reinheit aufzubauen. Benutzt mein Splitting.

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
Eintrag zu scikit-learn. Benutzt verschiedenste Methoden.

@misc{Dua:2019 ,
author = {Dua, Dheeru and Graff, Casey},
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }
Digits Datensatz

@book(breiman1984classification,
  title = {Classification and Regression Trees},
  author = {Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  isbn = {9780412048418},
  lccn = {83019708},
  url = {https://books.google.de/books?id=JwQx-WOmSyQC},
  year = 1984,
  publisher = {Taylor \& Francis}
)
Threshold per Minimum und Maximum Intervallen; introduction to Tree Classification

@article{soni2010implementation,
  title={Implementation of multivariate data set by CART algorithm},
  author={Soni, Sneha},
  journal={International Journal of Information Technology and Knowledge Management},
  volume={2},
  number={2},
  pages={455--459},
  year={2010}
}
Analyse des Iris Datasets mit Cart

@Article{Breiman2001,
author={Breiman, Leo},
title={Random Forests},
journal={Machine Learning},
year={2001},
month={Oct},
day={01},
volume={45},
number={1},
pages={5-32},
abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
issn={1573-0565},
doi={10.1023/A:1010933404324},
url={https://doi.org/10.1023/A:1010933404324}
}
Definition von Breiman zu RandomForest

@book{quinlan1993c45,
  added-at = {2008-02-29T15:06:40.000+0100},
  address = {San Francisco, CA, USA},
  author = {Quinlan, J. Ross},
  biburl = {https://www.bibsonomy.org/bibtex/2c0e213c325fbfc0d8ee914255cc4ee24/beate},
  description = {C4.5: programs for machine learning},
  interhash = {1a265267f55efc59cd96ecb93a69b520},
  intrahash = {c0e213c325fbfc0d8ee914255cc4ee24},
  isbn = {1-55860-238-0},
  keywords = {c4.5 machine-learning},
  publisher = {Morgan Kaufmann Publishers Inc.},
  timestamp = {2008-12-09T16:32:51.000+0100},
  title = {C4.5: programs for machine learning},
  url = {http://portal.acm.org/citation.cfm?id=152181},
  year = 1993
}
C4.5 Quinlan

@article{CRAWFORD1989197,
title = {Extensions to the CART algorithm},
journal = {International Journal of Man-Machine Studies},
volume = {31},
number = {2},
pages = {197-217},
year = {1989},
issn = {0020-7373},
doi = {https://doi.org/10.1016/0020-7373(89)90027-8},
url = {https://www.sciencedirect.com/science/article/pii/0020737389900278},
author = {Stuart L. Crawford},
abstract = {The CART concept induction algorithm recursively partitions the measurement space, displaying the resulting partitions as decision trees. Care, however, must be taken not to overfit the trees to the data, and CART employs cross-validation (cv) as the means by which an appropriately sized tree is selected. Although unbiased, cv estimates exhibit high variance, a troublesome characteristic, particularly for small learning sets. This paper describes Monte Carlo experiments which illustrate the effectiveness of the ·632 bootstrap as an alternative technique for tree selection and error estimation. In addition, a new incremental learning extension to CART is described.}
}
oberflächliche Beschreibung von Entscheidungsbäumen

@ARTICLE{Strobl2009-hw,
  title    = "An introduction to recursive partitioning: rationale,
              application, and characteristics of classification and regression
              trees, bagging, and random forests",
  author   = "Strobl, Carolin and Malley, James and Tutz, Gerhard",
  abstract = "Recursive partitioning methods have become popular and widely
              used tools for nonparametric regression and classification in
              many scientific fields. Especially random forests, which can deal
              with large numbers of predictor variables even in the presence of
              complex interactions, have been applied successfully in genetics,
              clinical medicine, and bioinformatics within the past few years.
              High-dimensional problems are common not only in genetics, but
              also in some areas of psychological research, where only a few
              subjects can be measured because of time or cost constraints, yet
              a large amount of data is generated for each subject. Random
              forests have been shown to achieve a high prediction accuracy in
              such applications and to provide descriptive variable importance
              measures reflecting the impact of each variable in both main
              effects and interactions. The aim of this work is to introduce
              the principles of the standard recursive partitioning methods as
              well as recent methodological improvements, to illustrate their
              usage for low and high-dimensional data exploration, but also to
              point out limitations of the methods and potential pitfalls in
              their practical application. Application of the methods is
              illustrated with freely available implementations in the R system
              for statistical computing.",
  journal  = "Psychol Methods",
  volume   =  14,
  number   =  4,
  pages    = "323--348",
  month    =  dec,
  year     =  2009,
  language = "en"
}
kurze Beschreibungen von Entscheidungsbaum und Random Forest und psycho ref

@inproceedings{lewis2000introduction,
  title={An introduction to classification and regression tree (CART) analysis},
  author={Lewis, Roger J},
  booktitle={Annual meeting of the society for academic emergency medicine in San Francisco, California},
  volume={14},
  year={2000},
  organization={Citeseer}
}
weitere Enführung in CART und klinische Studien ref

@article{sharma2016survey,
  title={A survey on decision tree algorithms of classification in data mining},
  author={Sharma, Himani and Kumar, Sunil},
  journal={International Journal of Science and Research (IJSR)},
  volume={5},
  number={4},
  pages={2094--2097},
  year={2016}
}
Klassifikation für Data-Mining

@Inbook{Hastie2009,
author="Hastie, Trevor
and Tibshirani, Robert
and Friedman, Jerome",
title="Random Forests",
bookTitle="The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
year="2009",
publisher="Springer New York",
address="New York, NY",
pages="587--604",
isbn="978-0-387-84858-7",
doi="10.1007/978-0-387-84858-7_15",
url="https://doi.org/10.1007/978-0-387-84858-7_15"
}
Anzahl Attribute Random Forest

@article{wielemaker:2011:tplp,
  author = {Wielemaker, Jan and Schrijvers, Tom and Triska, Markus and Lager, Torbj\"o{}rn},
  title = {{SWI-Prolog}},
  journal = {Theory and Practice of Logic Programming},
  year = {2012},
  volume = {12},
  number = {1-2},
  pages = {67--96},
  ISSN = {1471-0684},
  abstract = {SWI-Prolog is neither a commercial Prolog system nor a purely academic
enterprise, but increasingly a community project. The core system has
been shaped to its current form while being used as a tool for building
research prototypes, primarily for knowledge-intensive and
interactive systems. Community contributions have added
several interfaces and the constraint (CLP) libraries. Commercial
involvement has created the initial garbage collector, added several
interfaces and two development tools: PlDoc (a literate programming
documentation system) and PlUnit (a unit testing environment).

In this article we present SWI-Prolog as an integrating tool, supporting
a wide range of ideas developed in the Prolog community and acting as
glue between foreign resources. This article itself is the
glue between technical articles on SWI-Prolog, providing context and
experience in applying them over a longer period.}
}
SWI-Prolog

@book{carlsson1988sicstus,
  title={SICStus Prolog user's manual},
  author={Carlsson, Mats and Widen, Johan and Andersson, Johan and Andersson, Stefan and Boortz, Kent and Nilsson, Hans and Sj{\"o}land, Thomas},
  volume={3},
  number={1},
  year={1988},
  publisher={Swedish Institute of Computer Science Kista}
}
SICSTus Prolog

@inproceedings{Colmerauer1993TheBO,
  title={The birth of Prolog},
  author={Alain Colmerauer and Philippe Roussel},
  booktitle={HOPL-II},
  year={1993}
}
Birth of Prolog

@book{merritt2012building,
  title={Building expert systems in Prolog},
  author={Merritt, Dennis},
  year={2012},
  publisher={Springer Science \& Business Media}
}
Prolog Expert Systems

@conference{806816,
  author = {Changwei Liu and Anoop Singhal and Duminda Wijesekera},
  title = {A Logic Based Network Forensics Model for Evidence Analysis},
  year = {2015},
  number = {462},
  month = {2015-01-28 00:01:00},
  publisher = {Advances in Digital Forensics XI, Orlando, FL, US},
  url = {https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=918321},
  doi = {https://doi.org/10.1007/978-3-319-24123-4_8},
  language = {en},
}
Prolog benutzt für Cybersecurity

@inproceedings{wicaksono2016relational,
  title={Relational tool use learning by a robot in a real and simulated world},
  author={Wicaksono, Handy and Sammut, Claude},
  booktitle={Proceedings of ACRA},
  year={2016}
}
Prolog für Robotik

@book{shoham2014artificial,
  title={Artificial intelligence techniques in Prolog},
  author={Shoham, Yoav},
  year={2014},
  publisher={Morgan Kaufmann}
}
Prolog in AI

@article{rcolorbrewer2018package,
  title={Package ‘randomforest’},
  author={RColorBrewer, Suggests and Liaw, Maintainer Andy},
  journal={University of California, Berkeley: Berkeley, CA, USA},
  year={2018}
}
packet randomForest

@article{therneau2015package,
  title={Package ‘rpart’},
  author={Therneau, Terry and Atkinson, Beth and Ripley, Brian and Ripley, Maintainer Brian},
  journal={Available online: cran. ma. ic. ac. uk/web/packages/rpart/rpart. pdf (accessed on 20 April 2016)},
  year={2015}
}
packet rpart

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer}
}
Bagging Paper

@inproceedings{zombori2020prolog,
  title={Prolog technology reinforcement learning prover},
  author={Zombori, Zsolt and Urban, Josef and Brown, Chad E},
  booktitle={International Joint Conference on Automated Reasoning},
  pages={489--507},
  year={2020},
  organization={Springer}
}
Prolog als Theorem Prover mit Reinforcement Learning

@inproceedings{sridhar2010actionscript,
  title={ActionScript in-lined reference monitoring in Prolog},
  author={Sridhar, Meera and Hamlen, Kevin W},
  booktitle={International Symposium on Practical Aspects of Declarative Languages},
  pages={149--151},
  year={2010},
  organization={Springer}
}
Prolog für Model Checking

@article{pace1997sparse,
  title={Sparse spatial autoregressions},
  author={Pace, R Kelley and Barry, Ronald},
  journal={Statistics \& Probability Letters},
  volume={33},
  number={3},
  pages={291--297},
  year={1997},
  publisher={Elsevier}
}
California Housing

@article{fisher1936use,
  title={The use of multiple measurements in taxonomic problems},
  author={Fisher, Ronald A},
  journal={Annals of eugenics},
  volume={7},
  number={2},
  pages={179--188},
  year={1936},
  publisher={Wiley Online Library}
}
Iris Datensatz

@article{efron2004least,
  title={Least angle regression},
  author={Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  journal={The Annals of statistics},
  volume={32},
  number={2},
  pages={407--499},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}
Diabetes Datensatz

@article{mckinney2011pandas,
  title={pandas: a foundational Python library for data analysis and statistics},
  author={McKinney, Wes and others},
  journal={Python for high performance and scientific computing},
  volume={14},
  number={9},
  pages={1--9},
  year={2011},
  publisher={Seattle}
}
pandas Bibliothek

@misc{izrailev2014tictoc,
  title={tictoc: Functions for timing R scripts, as well as implementations of Stack and List structures.. R package version 1.0},
  author={Izrailev, Sergei},
  year={2014}
}
tictoc packet